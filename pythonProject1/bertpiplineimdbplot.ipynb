{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13522720,
     "sourceType": "datasetVersion",
     "datasetId": 8586397
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "86e590ce69d4e40b",
   "cell_type": "markdown",
   "source": "Fonction pour merge les fichiers parquet de HF",
   "metadata": {}
  },
  {
   "id": "6e0b9743516a1769",
   "cell_type": "markdown",
   "source": "Merge les données",
   "metadata": {}
  },
  {
   "id": "89dedcd8c909bf22",
   "cell_type": "markdown",
   "source": "Création du DS HF",
   "metadata": {}
  },
  {
   "id": "127b0e1be6fe3e34",
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"parquet\", data_files=\"data/mmimdb_merged.parquet\", split=\"train\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:09:29.655396Z",
     "iopub.execute_input": "2025-11-06T13:09:29.655852Z",
     "iopub.status.idle": "2025-11-06T13:10:10.358278Z",
     "shell.execute_reply.started": "2025-11-06T13:09:29.655807Z",
     "shell.execute_reply": "2025-11-06T13:10:10.357371Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:27:51.190026Z",
     "start_time": "2025-11-06T13:27:39.529166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "id": "aedc9d2b-0daa-40a9-9ecf-f5bc7c4f5b9b",
   "cell_type": "code",
   "source": [
    "def flatten_text_only(example):\n",
    "\n",
    "    import json\n",
    "    msgs = example.get(\"messages\", []) or []\n",
    "\n",
    "    user_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"]\n",
    "    assistant_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"]\n",
    "\n",
    "    prompt = \"\\n\\n\".join([u for u in user_msgs if isinstance(u, str)]) if user_msgs else \"\"\n",
    "    answer = assistant_msgs[-1] if assistant_msgs else \"\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": answer,\n",
    "    }\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:10.360723Z",
     "iopub.execute_input": "2025-11-06T13:10:10.361274Z",
     "iopub.status.idle": "2025-11-06T13:10:10.368873Z",
     "shell.execute_reply.started": "2025-11-06T13:10:10.361244Z",
     "shell.execute_reply": "2025-11-06T13:10:10.368165Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:27:51.229657Z",
     "start_time": "2025-11-06T13:27:51.226595Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "e1394bcb9c3036fd",
   "cell_type": "code",
   "source": [
    "print(ds)\n",
    "ds_flat = ds.map(flatten_text_only, remove_columns=ds.column_names)\n",
    "df = ds_flat.to_pandas()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:10.369447Z",
     "iopub.execute_input": "2025-11-06T13:10:10.369693Z",
     "iopub.status.idle": "2025-11-06T13:10:15.477825Z",
     "shell.execute_reply.started": "2025-11-06T13:10:10.369669Z",
     "shell.execute_reply": "2025-11-06T13:10:15.476882Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:16.325295Z",
     "start_time": "2025-11-06T13:28:15.468940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages', 'images'],\n",
      "    num_rows: 15552\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15552/15552 [00:00<00:00, 18819.21 examples/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "id": "f8a4e510b36657e",
   "cell_type": "code",
   "source": "print(ds_flat[0])\nprint(df.count().isnull())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:15.481755Z",
     "iopub.execute_input": "2025-11-06T13:10:15.482521Z",
     "iopub.status.idle": "2025-11-06T13:10:15.498891Z",
     "shell.execute_reply.started": "2025-11-06T13:10:15.482478Z",
     "shell.execute_reply": "2025-11-06T13:10:15.498250Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:22.075872Z",
     "start_time": "2025-11-06T13:28:22.068383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Given the movie poster and the corresponding plot of the movie, choose the appropriategenres from the following comma separated genres: drama, comedy, romance, thriller, crime, action, adventure,horror, documentry, mystery, sci-fi, fantasy, family, biography, war, history, music, animation, musical,western, sport, short, film-noir.\\nPlot: Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.\\nNote that a movie can belong to more than one genres,provide all the suitable genres separated by commas. Answer:', 'answer': 'Crime, Drama, Thriller'}\n",
      "prompt    False\n",
      "answer    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "id": "9cd816fe4d41fc3f",
   "cell_type": "code",
   "source": "print(ds_flat)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:15.499440Z",
     "iopub.execute_input": "2025-11-06T13:10:15.499674Z",
     "iopub.status.idle": "2025-11-06T13:10:17.052131Z",
     "shell.execute_reply.started": "2025-11-06T13:10:15.499652Z",
     "shell.execute_reply": "2025-11-06T13:10:17.051250Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:26.392507Z",
     "start_time": "2025-11-06T13:28:26.390019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 15552\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "id": "5d0fdef18e40489a",
   "cell_type": "code",
   "source": "print(ds_flat[0]['prompt'])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:17.052988Z",
     "iopub.execute_input": "2025-11-06T13:10:17.054245Z",
     "iopub.status.idle": "2025-11-06T13:10:17.888389Z",
     "shell.execute_reply.started": "2025-11-06T13:10:17.054225Z",
     "shell.execute_reply": "2025-11-06T13:10:17.887366Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:29.505635Z",
     "start_time": "2025-11-06T13:28:29.502487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the movie poster and the corresponding plot of the movie, choose the appropriategenres from the following comma separated genres: drama, comedy, romance, thriller, crime, action, adventure,horror, documentry, mystery, sci-fi, fantasy, family, biography, war, history, music, animation, musical,western, sport, short, film-noir.\n",
      "Plot: Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.\n",
      "Note that a movie can belong to more than one genres,provide all the suitable genres separated by commas. Answer:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "id": "ba65df49c34affb6",
   "cell_type": "code",
   "source": "#Garde seulement le prompt ce qui permet d'optimiser l'entrainement aussi bien en temps qu'en perf\nimport re\n\n# Ne garde que le texte du plot\nPATTERN = re.compile(r'(?is)\\bplot\\s*:\\s*(.*?)(?=\\bnote\\s+that\\b|answer\\s*:|$)')\n\ndef extract_plot(text: str) -> str:\n    if text is None:\n        return None\n    m = PATTERN.search(text)\n    if not m:\n        return \"\"  # ou text.strip() si tu préfères conserver l'original quand Pas de \"Plot:\"\n    plot = m.group(1).strip()\n    # petites normalisations\n    plot = re.sub(r'\\s+\\n', '\\n', plot)\n    plot = re.sub(r'\\s{2,}', ' ', plot)\n    return plot\n\n# Option A : créer une colonne 'plot'\nds_flat = ds_flat.map(lambda batch: {\"plot\": [extract_plot(t) for t in batch[\"prompt\"]]},\n                      batched=True)\n\n# Option B : remplacer 'prompt' par le seul plot\n# ds_flat = ds_flat.map(lambda batch: {\"prompt\": [extract_plot(t) for t in batch[\"prompt\"]]},\n#                       batched=True)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:17.889298Z",
     "iopub.execute_input": "2025-11-06T13:10:17.889597Z",
     "iopub.status.idle": "2025-11-06T13:10:19.948476Z",
     "shell.execute_reply.started": "2025-11-06T13:10:17.889575Z",
     "shell.execute_reply": "2025-11-06T13:10:19.947501Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:32.169281Z",
     "start_time": "2025-11-06T13:28:31.454993Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15552/15552 [00:00<00:00, 22082.53 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "id": "efe941a34455fee6",
   "cell_type": "code",
   "source": "print(ds_flat[0]['plot'])\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:19.949541Z",
     "iopub.execute_input": "2025-11-06T13:10:19.949823Z",
     "iopub.status.idle": "2025-11-06T13:10:19.955289Z",
     "shell.execute_reply.started": "2025-11-06T13:10:19.949800Z",
     "shell.execute_reply": "2025-11-06T13:10:19.954693Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:34.228248Z",
     "start_time": "2025-11-06T13:28:34.224986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "id": "1246a89d554d4047",
   "cell_type": "markdown",
   "source": "Partie en ds",
   "metadata": {}
  },
  {
   "id": "af1944a4306b3ef1",
   "cell_type": "code",
   "source": "import re\nfrom datasets import Features, Sequence, Value\n\nLABELS = [\n    \"drama\",\"comedy\",\"romance\",\"thriller\",\"crime\",\"action\",\"adventure\",\"horror\",\n    \"documentary\",\"mystery\",\"sci-fi\",\"fantasy\",\"family\",\"biography\",\"war\",\"history\",\n    \"music\",\"animation\",\"musical\",\"western\",\"sport\",\"short\",\"film-noir\"\n]\nlabel2id = {lab:i for i,lab in enumerate(LABELS)}\n\nALIASES = {\n    \"documentry\": \"documentary\",\n    \"science fiction\": \"sci-fi\",\n    \"sci fi\": \"sci-fi\",\n    \"film noir\": \"film-noir\",\n    \"westerns\": \"western\",\n    \"sports\": \"sport\",\n}\n\ndef norm_token(t: str) -> str:\n    t = t.lower()\n    t = re.sub(r\"[\\s\\-]+\", \" \", t).strip()\n    t = ALIASES.get(t, t)\n    t = t.replace(\"sci fi\", \"sci-fi\")\n    return t\n\ndef parse_answer(ans: str):\n    if ans is None:\n        return set()\n    toks = re.split(r\"[;,]\", ans)\n    toks = [norm_token(x) for x in toks if x.strip()]\n    return set(t for t in toks if t in label2id)\n\ndef to_multi_hot_float(labels_set):\n    vec = [0.0]*len(LABELS)\n    for lab in labels_set:\n        vec[label2id[lab]] = 1.0\n    return vec\n\ndef mapper(batch):\n    answers = batch[\"answer\"]\n    return {\"labels\": [to_multi_hot_float(parse_answer(a)) for a in answers]}\n\nnew_features = ds_flat.features.copy()\nnew_features[\"labels\"] = Sequence(feature=Value(\"float32\"), length=len(LABELS))\n\nds_flat = ds_flat.map(mapper, batched=True, features=new_features)\nprint(type(ds_flat[0][\"labels\"][0]), ds_flat.features[\"labels\"])\n# -> <class 'float'> Sequence(feature=Value(dtype='float32', id=None), length=23, id=None)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:19.955881Z",
     "iopub.execute_input": "2025-11-06T13:10:19.956110Z",
     "iopub.status.idle": "2025-11-06T13:10:20.196269Z",
     "shell.execute_reply.started": "2025-11-06T13:10:19.956090Z",
     "shell.execute_reply": "2025-11-06T13:10:20.195520Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:36.599446Z",
     "start_time": "2025-11-06T13:28:36.352136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15552/15552 [00:00<00:00, 70392.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> List(Value('float32'), length=23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "id": "dda2ff4847f945ae",
   "cell_type": "code",
   "source": "print(ds_flat[0][\"labels\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:20.196917Z",
     "iopub.execute_input": "2025-11-06T13:10:20.197174Z",
     "iopub.status.idle": "2025-11-06T13:10:20.201573Z",
     "shell.execute_reply.started": "2025-11-06T13:10:20.197151Z",
     "shell.execute_reply": "2025-11-06T13:10:20.200879Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:40.402827Z",
     "start_time": "2025-11-06T13:28:40.399530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "id": "4e10be063a5ab597",
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\n\n# Définition du tokenizer\nmodel_name = \"bert-base-uncased\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name) \n\ndef tokenize_and_count(example):\n    # Tokenisation sans padding ni truncation\n    plot_tokens = tokenizer(example['plot'], add_special_tokens=False)['input_ids']\n\n    # Comptage\n    num_title_tokens = len(plot_tokens)\n    total_tokens = num_title_tokens\n\n    # Retour sous forme de colonnes\n    return {\n        \"num_title_tokens\": num_title_tokens,\n        \"num_tokens_total\": total_tokens\n    }\n\n# Application sur tout le dataset (batched=False car une instance à la fois)\ncountTokenize = ds_flat.map(tokenize_and_count)\n\n# Vérification\nprint(countTokenize['plot'][0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:20.202126Z",
     "iopub.execute_input": "2025-11-06T13:10:20.202368Z",
     "iopub.status.idle": "2025-11-06T13:10:39.026883Z",
     "shell.execute_reply.started": "2025-11-06T13:10:20.202344Z",
     "shell.execute_reply": "2025-11-06T13:10:39.025866Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:55.558849Z",
     "start_time": "2025-11-06T13:28:43.747518Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Utilisateur\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map:   0%|          | 0/15552 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 15552/15552 [00:04<00:00, 3209.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "id": "f745f99975646332",
   "cell_type": "code",
   "source": "print(countTokenize[0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:39.027696Z",
     "iopub.execute_input": "2025-11-06T13:10:39.027969Z",
     "iopub.status.idle": "2025-11-06T13:10:39.032907Z",
     "shell.execute_reply.started": "2025-11-06T13:10:39.027945Z",
     "shell.execute_reply": "2025-11-06T13:10:39.032065Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:55.618851Z",
     "start_time": "2025-11-06T13:28:55.615641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Given the movie poster and the corresponding plot of the movie, choose the appropriategenres from the following comma separated genres: drama, comedy, romance, thriller, crime, action, adventure,horror, documentry, mystery, sci-fi, fantasy, family, biography, war, history, music, animation, musical,western, sport, short, film-noir.\\nPlot: Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.\\nNote that a movie can belong to more than one genres,provide all the suitable genres separated by commas. Answer:', 'answer': 'Crime, Drama, Thriller', 'plot': 'Mild mannered businessman Anthony Wongs life is shattered when his pregnant wife is run over by a busy taxi driver. This and another incident with a sleazy cab driver causes Wong to go on a mission to kill bad taxi drivers.', 'labels': [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'num_title_tokens': 47, 'num_tokens_total': 47}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "id": "b24b7de31d0962ae",
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\ntoken_counts = countTokenize[\"num_tokens_total\"]  # ou \"validation\", \"test\" selon le cas\n\nplt.hist(token_counts, bins=50)\nplt.title(\"Distribution du nombre de tokens (input_ids)\")\nplt.xlabel(\"Nombre de tokens\")\nplt.ylabel(\"Nombre d'exemples\")\nplt.grid(True)\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:39.033771Z",
     "iopub.execute_input": "2025-11-06T13:10:39.034186Z",
     "iopub.status.idle": "2025-11-06T13:10:39.646515Z",
     "shell.execute_reply.started": "2025-11-06T13:10:39.034160Z",
     "shell.execute_reply": "2025-11-06T13:10:39.645640Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:56.641261Z",
     "start_time": "2025-11-06T13:28:55.697861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "id": "8f6ded01c663595",
   "cell_type": "code",
   "source": "import numpy as np\n\ntoken_counts = countTokenize[\"num_tokens_total\"] # ou autre split\n\npercentile_90 = int(np.percentile(token_counts, 90))\nprint(f\"90% des exemples ont une longueur ≤ {percentile_90} tokens\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:39.647292Z",
     "iopub.execute_input": "2025-11-06T13:10:39.647545Z",
     "iopub.status.idle": "2025-11-06T13:10:39.849074Z",
     "shell.execute_reply.started": "2025-11-06T13:10:39.647528Z",
     "shell.execute_reply": "2025-11-06T13:10:39.848280Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:56.822175Z",
     "start_time": "2025-11-06T13:28:56.663228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% des exemples ont une longueur ≤ 191 tokens\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "id": "360f2bc55c04402c",
   "cell_type": "code",
   "source": "MAX_LEN = 224\nprint(MAX_LEN)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:39.849855Z",
     "iopub.execute_input": "2025-11-06T13:10:39.850107Z",
     "iopub.status.idle": "2025-11-06T13:10:39.855303Z",
     "shell.execute_reply.started": "2025-11-06T13:10:39.850087Z",
     "shell.execute_reply": "2025-11-06T13:10:39.854593Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:28:56.876499Z",
     "start_time": "2025-11-06T13:28:56.873471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "id": "ef04c15765f2379e",
   "cell_type": "code",
   "source": "# Tokenisation sans padding ni truncation\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"plot\"],\n                     padding=\"max_length\",\n                     truncation=True )\n\ntokenized_datasets = ds_flat.map(tokenize_function, batched=True)\n\ntokenizer.save_pretrained(\"../dataset/mon_tokenizer\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:39.856125Z",
     "iopub.execute_input": "2025-11-06T13:10:39.856450Z",
     "iopub.status.idle": "2025-11-06T13:10:46.653373Z",
     "shell.execute_reply.started": "2025-11-06T13:10:39.856429Z",
     "shell.execute_reply": "2025-11-06T13:10:46.652600Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:29:00.382089Z",
     "start_time": "2025-11-06T13:28:57.094708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15552/15552 [00:03<00:00, 4770.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../dataset/mon_tokenizer\\\\tokenizer_config.json',\n",
       " '../dataset/mon_tokenizer\\\\special_tokens_map.json',\n",
       " '../dataset/mon_tokenizer\\\\vocab.txt',\n",
       " '../dataset/mon_tokenizer\\\\added_tokens.json',\n",
       " '../dataset/mon_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "id": "74a685937c64e97c",
   "cell_type": "code",
   "source": "from datasets import DatasetDict\n\n# Split à 90% train / 20% test\ndataset = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n\ntrain_dataset = dataset[\"train\"].shuffle(seed=42)\neval_dataset = dataset[\"test\"].shuffle(seed=42)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:46.654255Z",
     "iopub.execute_input": "2025-11-06T13:10:46.654651Z",
     "iopub.status.idle": "2025-11-06T13:10:46.693617Z",
     "shell.execute_reply.started": "2025-11-06T13:10:46.654623Z",
     "shell.execute_reply": "2025-11-06T13:10:46.692901Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:29:00.422312Z",
     "start_time": "2025-11-06T13:29:00.394137Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "id": "c91f54ab71caee51",
   "cell_type": "code",
   "source": "import numpy as np\n\n# Convertir tous les labels en tableau numpy (n_samples, 6)\nlabel_array = np.array(train_dataset['labels'])\n\n# Somme par colonne (i.e., par classe)\nlabel_sums = label_array.sum(axis=0)\n\n# Affichage\nprint(\"Répartition multilabel dans train_dataset :\")\nfor idx, count in enumerate(label_sums):\n    print(f\"Label {idx}: {int(count)} occurrences\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:46.694412Z",
     "iopub.execute_input": "2025-11-06T13:10:46.694661Z",
     "iopub.status.idle": "2025-11-06T13:10:47.696780Z",
     "shell.execute_reply.started": "2025-11-06T13:10:46.694640Z",
     "shell.execute_reply": "2025-11-06T13:10:47.695990Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:29:01.238158Z",
     "start_time": "2025-11-06T13:29:00.444468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition multilabel dans train_dataset :\n",
      "Label 0: 6710 occurrences\n",
      "Label 1: 4081 occurrences\n",
      "Label 2: 2589 occurrences\n",
      "Label 3: 2465 occurrences\n",
      "Label 4: 1824 occurrences\n",
      "Label 5: 1688 occurrences\n",
      "Label 6: 1284 occurrences\n",
      "Label 7: 1272 occurrences\n",
      "Label 8: 998 occurrences\n",
      "Label 9: 972 occurrences\n",
      "Label 10: 959 occurrences\n",
      "Label 11: 948 occurrences\n",
      "Label 12: 790 occurrences\n",
      "Label 13: 631 occurrences\n",
      "Label 14: 655 occurrences\n",
      "Label 15: 548 occurrences\n",
      "Label 16: 514 occurrences\n",
      "Label 17: 463 occurrences\n",
      "Label 18: 408 occurrences\n",
      "Label 19: 341 occurrences\n",
      "Label 20: 301 occurrences\n",
      "Label 21: 225 occurrences\n",
      "Label 22: 159 occurrences\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "id": "d174de9458b37d76",
   "cell_type": "code",
   "source": "import numpy as np\n\n# Convertir tous les labels en tableau numpy (n_samples, 6)\nlabel_array = np.array(eval_dataset['labels'])\n\n# Somme par colonne (i.e., par classe)\nlabel_sums = label_array.sum(axis=0)\n\n# Affichage\nprint(\"Répartition multilabel dans train_dataset :\")\nfor idx, count in enumerate(label_sums):\n    print(f\"Label {idx}: {int(count)} occurrences\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:47.700857Z",
     "iopub.execute_input": "2025-11-06T13:10:47.701073Z",
     "iopub.status.idle": "2025-11-06T13:10:47.899603Z",
     "shell.execute_reply.started": "2025-11-06T13:10:47.701057Z",
     "shell.execute_reply": "2025-11-06T13:10:47.898826Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:29:01.477126Z",
     "start_time": "2025-11-06T13:29:01.326465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition multilabel dans train_dataset :\n",
      "Label 0: 1714 occurrences\n",
      "Label 1: 1027 occurrences\n",
      "Label 2: 637 occurrences\n",
      "Label 3: 648 occurrences\n",
      "Label 4: 469 occurrences\n",
      "Label 5: 467 occurrences\n",
      "Label 6: 327 occurrences\n",
      "Label 7: 331 occurrences\n",
      "Label 8: 236 occurrences\n",
      "Label 9: 259 occurrences\n",
      "Label 10: 253 occurrences\n",
      "Label 11: 214 occurrences\n",
      "Label 12: 188 occurrences\n",
      "Label 13: 157 occurrences\n",
      "Label 14: 151 occurrences\n",
      "Label 15: 132 occurrences\n",
      "Label 16: 120 occurrences\n",
      "Label 17: 123 occurrences\n",
      "Label 18: 95 occurrences\n",
      "Label 19: 82 occurrences\n",
      "Label 20: 78 occurrences\n",
      "Label 21: 56 occurrences\n",
      "Label 22: 43 occurrences\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "id": "80e799013cca1ce6",
   "cell_type": "code",
   "source": "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport numpy as np\nimport torch\n\ndef compute_metrics(pred):\n    logits, labels = pred.predictions, pred.label_ids\n\n    # Convert logits to probabilities via sigmoid\n    probs = torch.sigmoid(torch.from_numpy(logits)).numpy()\n\n    # Binarisation avec seuil 0.5\n    preds = (probs > 0.4).astype(int)\n\n    # Retourne les métriques arrondies à 4 décimales\n    return {\n        \"exact_match_accuracy\": round(accuracy_score(labels, preds), 2),\n        \"label_accuracy\": round((preds == labels).mean(), 2),\n        \"f1_micro\": round(f1_score(labels, preds, average=\"micro\", zero_division=0), 2),\n        \"f1_macro\": round(f1_score(labels, preds, average=\"macro\", zero_division=0), 2),\n        \"precision_micro\": round(precision_score(labels, preds, average=\"micro\", zero_division=0), 2),\n        \"recall_micro\": round(recall_score(labels, preds, average=\"micro\", zero_division=0), 2),\n    }",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:47.900449Z",
     "iopub.execute_input": "2025-11-06T13:10:47.900930Z",
     "iopub.status.idle": "2025-11-06T13:10:47.906863Z",
     "shell.execute_reply.started": "2025-11-06T13:10:47.900907Z",
     "shell.execute_reply": "2025-11-06T13:10:47.905956Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:32:46.751713Z",
     "start_time": "2025-11-06T13:32:46.401128Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "id": "1e4d4758ac39e8bd",
   "cell_type": "code",
   "source": "\nfrom transformers import BertConfig, BertForSequenceClassification\nN_LABELS = 23\nconfig = BertConfig.from_pretrained('bert-base-uncased')\nconfig.hidden_dropout_prob = 0.3\nconfig.attention_probs_dropout_prob = 0.3\nconfig.classifier_dropout = 0.3\nconfig.num_labels = N_LABELS\nconfig.problem_type = \"multi_label_classification\"\n\n# Initialiser le modèle avec cette configuration\nfrom transformers import BertForSequenceClassification, BertConfig\n\n# Crée une config avec 6 labels et le bon type de problème\nconfig = BertConfig.from_pretrained(\n    'bert-base-uncased',\n    num_labels=N_LABELS,\n    problem_type=\"multi_label_classification\"\n)\n\n# Charge le modèle avec cette config\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    config=config\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:10:47.907814Z",
     "iopub.execute_input": "2025-11-06T13:10:47.908069Z",
     "iopub.status.idle": "2025-11-06T13:11:05.255505Z",
     "shell.execute_reply.started": "2025-11-06T13:10:47.908046Z",
     "shell.execute_reply": "2025-11-06T13:11:05.254898Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:36:26.462885Z",
     "start_time": "2025-11-06T13:32:49.892079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "id": "32fee1f1f1ccfaaf",
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:05.256203Z",
     "iopub.execute_input": "2025-11-06T13:11:05.256818Z",
     "iopub.status.idle": "2025-11-06T13:11:18.153176Z",
     "shell.execute_reply.started": "2025-11-06T13:11:05.256797Z",
     "shell.execute_reply": "2025-11-06T13:11:18.151706Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-06T13:37:12.291Z",
     "start_time": "2025-11-06T13:36:26.474284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Utilisateur\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "wandb: Currently logged in as: robinmg1806 (robinmg1806-esaip) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Utilisateur\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\wandb\\run-20251106_143631-t8f4pqr1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/robinmg1806-esaip/huggingface/runs/t8f4pqr1' target=\"_blank\">fine-water-46</a></strong> to <a href='https://wandb.ai/robinmg1806-esaip/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/robinmg1806-esaip/huggingface' target=\"_blank\">https://wandb.ai/robinmg1806-esaip/huggingface</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/robinmg1806-esaip/huggingface/runs/t8f4pqr1' target=\"_blank\">https://wandb.ai/robinmg1806-esaip/huggingface/runs/t8f4pqr1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m      3\u001B[39m training_args = TrainingArguments(\n\u001B[32m      4\u001B[39m     learning_rate=\u001B[32m5e-5\u001B[39m,\n\u001B[32m      5\u001B[39m     per_device_train_batch_size=\u001B[32m64\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m      9\u001B[39m     report_to=\u001B[33m\"\u001B[39m\u001B[33mwandb\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     10\u001B[39m )\n\u001B[32m     11\u001B[39m trainer = Trainer(\n\u001B[32m     12\u001B[39m     model=model,\n\u001B[32m     13\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     16\u001B[39m     compute_metrics=compute_metrics,\n\u001B[32m     17\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2667\u001B[39m context = (\n\u001B[32m   2668\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2670\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2671\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2672\u001B[39m )\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2674\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4020\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   4017\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   4019\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m4020\u001B[39m     loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4022\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   4023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   4024\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4025\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   4026\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4110\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   4108\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   4109\u001B[39m     inputs = {**inputs, **kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m4110\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4111\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   4112\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   4113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1482\u001B[39m, in \u001B[36mBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1474\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1475\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m   1476\u001B[39m \u001B[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m   1477\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m   1478\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1479\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1480\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1482\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1483\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1484\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1485\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1486\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1487\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1488\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1489\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1490\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1491\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1492\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1494\u001B[39m pooled_output = outputs[\u001B[32m1\u001B[39m]\n\u001B[32m   1496\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.dropout(pooled_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1000\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    993\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m    994\u001B[39m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[32m    995\u001B[39m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[32m    996\u001B[39m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[32m    997\u001B[39m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[32m    998\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m-> \u001B[39m\u001B[32m1000\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1014\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:650\u001B[39m, in \u001B[36mBertEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    646\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m    648\u001B[39m layer_head_mask = head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m650\u001B[39m layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    651\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    654\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    655\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    657\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    659\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    661\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    662\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:588\u001B[39m, in \u001B[36mBertLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001B[39m\n\u001B[32m    585\u001B[39m     attention_output = cross_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    586\u001B[39m     outputs = outputs + cross_attention_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add cross attentions if we output attention weights\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m layer_output = \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m outputs = (layer_output,) + outputs\n\u001B[32m    593\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001B[39m, in \u001B[36mapply_chunking_to_forward\u001B[39m\u001B[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[39m\n\u001B[32m    254\u001B[39m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001B[32m--> \u001B[39m\u001B[32m257\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:596\u001B[39m, in \u001B[36mBertLayer.feed_forward_chunk\u001B[39m\u001B[34m(self, attention_output)\u001B[39m\n\u001B[32m    595\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[32m--> \u001B[39m\u001B[32m596\u001B[39m     intermediate_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    597\u001B[39m     layer_output = \u001B[38;5;28mself\u001B[39m.output(intermediate_output, attention_output)\n\u001B[32m    598\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:513\u001B[39m, in \u001B[36mBertIntermediate.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    511\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001B[32m    512\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.dense(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m513\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mintermediate_act_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Groupe ESAIP\\Bureau\\Cetic_code\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\activations.py:85\u001B[39m, in \u001B[36mGELUActivation.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "id": "eaa3840a-801f-45e4-b685-61987e2ae084",
   "cell_type": "code",
   "source": [
    "import json\n",
    "# --- Sauvegarde tête seule ---\n",
    "head_path = \"data/classifier_head.pt\"\n",
    "meta_path = \"data/classifier_head_meta.json\"\n",
    "\n",
    "torch.save(model.classifier.state_dict(), head_path)\n",
    "\n",
    "meta = {\n",
    "    \"num_labels\": model.config.num_labels,\n",
    "    \"hidden_size\": model.config.hidden_size,\n",
    "    \"classifier_type\": type(model.classifier).__name__,  # \"BertOnlyMLMHead\" ? non, ici typiquement \"Linear\"\n",
    "    \"dropout_in_config\": getattr(model.config, \"classifier_dropout\", None),\n",
    "}\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.187649Z",
     "iopub.status.idle": "2025-11-06T13:11:18.188019Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.187820Z",
     "shell.execute_reply": "2025-11-06T13:11:18.187837Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2bb0d7cce63aa599",
   "cell_type": "markdown",
   "source": "Etude effectuer",
   "metadata": {}
  },
  {
   "id": "7a413db1-e1b1-4114-90df-667a775e8596",
   "cell_type": "code",
   "source": "eval_results = trainer.evaluate()\nprint(eval_results)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.153746Z",
     "iopub.status.idle": "2025-11-06T13:11:18.153987Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.153875Z",
     "shell.execute_reply": "2025-11-06T13:11:18.153885Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "501f37b3-49e2-4d30-ae74-be015c6b818a",
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef predict_multilabel(text, model, tokenizer, threshold=0.5, label_names=None):\n    model.eval()\n\n    device = next(model.parameters()).device  # récupère le device du modèle\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits  # shape: [1, num_labels]\n\n    probs = sigmoid(logits.cpu().numpy())[0]  # passage sur CPU pour numpy + sigmoid\n    predictions = (probs >= threshold).astype(int)\n\n    # indices triés par proba décroissante\n    order = np.argsort(-probs)\n\n    results = {}\n    for i in order:\n        label = label_names[i] if label_names else f\"label_{i}\"\n        results[label] = {\n            \"score\": float(probs[i]),\n            \"predicted\": bool(predictions[i])\n        }\n\n    return results",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.155230Z",
     "iopub.status.idle": "2025-11-06T13:11:18.155599Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.155417Z",
     "shell.execute_reply": "2025-11-06T13:11:18.155431Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "daeb4485-050d-4c1f-926f-ec1cf347683d",
   "cell_type": "code",
   "source": "from IPython.display import display\n\nimg = eval_dataset[\"images_json\"][0]   # retourne un objet PIL.Image.Image\ndisplay(img)                     # ou juste `img` en notebook",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.156105Z",
     "iopub.status.idle": "2025-11-06T13:11:18.156422Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.156246Z",
     "shell.execute_reply": "2025-11-06T13:11:18.156259Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "9c01bae1-c852-4b3e-9036-495d3886c279",
   "cell_type": "code",
   "source": "print(eval_dataset[a]['labels'])\nprint(eval_dataset[a]['answer'])\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.156923Z",
     "iopub.status.idle": "2025-11-06T13:11:18.157222Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.157063Z",
     "shell.execute_reply": "2025-11-06T13:11:18.157076Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "299a3f4b-4a57-4d18-9ea0-617028e7d679",
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\nimport random\na=random.randint(0, 3110)\nprint(eval_dataset[a]['plot'])\nprint(\"resultat = \" +eval_dataset[a]['answer'])\ntext = eval_dataset[a]['plot']\nlabel_names = LABELS\n\nresults = predict_multilabel(text, model, tokenizer, threshold=0.5, label_names=label_names)\n\nfor label, info in results.items():\n    print(f\"{label:10} | score: {info['score']:.4f} | prédiction: {'✔️' if info['predicted'] else '❌'}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.158667Z",
     "iopub.status.idle": "2025-11-06T13:11:18.159003Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.158829Z",
     "shell.execute_reply": "2025-11-06T13:11:18.158843Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2f81005e-4538-4067-a1bb-56933a3e9b94",
   "cell_type": "code",
   "source": "from sklearn.metrics import multilabel_confusion_matrix\nimport numpy as np\n\npred = trainer.predict(eval_dataset)\nlogits = pred.predictions               # shape (n_samples, n_labels)\ny_true = pred.label_ids                 # 0/1\n\ny_prob = 1/(1+np.exp(-logits))          # sigmoïde\ny_pred = (y_prob >= 0.5).astype(int)\n\nml_cm = multilabel_confusion_matrix(y_true, y_pred)  # (n_labels, 2, 2)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.160005Z",
     "iopub.status.idle": "2025-11-06T13:11:18.160322Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.160138Z",
     "shell.execute_reply": "2025-11-06T13:11:18.160150Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "d835905d-13bf-4a2c-a730-aad147f845e8",
   "cell_type": "code",
   "source": "import numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report\n\n# y_true: array (n_samples, n_labels) de 0/1\n# y_prob: array (n_samples, n_labels) de proba dans [0,1] (ou logits après sigmoid)\nthreshold = 0.5\ny_pred = (y_prob >= threshold).astype(int)\n\nml_cm = multilabel_confusion_matrix(y_true, y_pred)  # shape: (n_labels, 2, 2)\n# ml_cm[i] = [[TN, FP],\n#             [FN, TP]] pour le label i\n\n# Afficher une matrice pour un label donné (ex: i=0)\nimport matplotlib.pyplot as plt\n\ni = 0  # index du label à visualiser\nfig, ax = plt.subplots()\nim = ax.imshow(ml_cm[i], cmap=\"Blues\")\nax.set_xticks([0,1]); ax.set_xticklabels([\"Négatif\", \"Positif\"])\nax.set_yticks([0,1]); ax.set_yticklabels([\"Négatif\", \"Positif\"])\nax.set_xlabel(\"Prédit\"); ax.set_ylabel(\"Vrai\")\nfor (r,c), val in np.ndenumerate(ml_cm[i]):\n    ax.text(c, r, int(val), ha=\"center\", va=\"center\")\nplt.title(\"Matrice de confusion — label %d\" % i)\nplt.colorbar(im, ax=ax)\nplt.tight_layout(); plt.show()\n\n# Rapport utile par label (precision/recall/F1 micro/macro)\nprint(classification_report(y_true, y_pred, target_names=label_names))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.162444Z",
     "iopub.status.idle": "2025-11-06T13:11:18.162766Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.162600Z",
     "shell.execute_reply": "2025-11-06T13:11:18.162613Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "15f3d5c5-1082-453d-8683-e4abcb0753e3",
   "cell_type": "code",
   "source": "zaeae",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.163250Z",
     "iopub.status.idle": "2025-11-06T13:11:18.163558Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.163405Z",
     "shell.execute_reply": "2025-11-06T13:11:18.163419Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "9771de2d21ad01db",
   "cell_type": "code",
   "source": "# Supprimer les textes inutiles\ns = df['prompt'].astype(str).str.replace('\\u00A0', ' ', regex=False)\n\n# coupe tout ce qui est avant \"Plot:\" (insensible à la casse) ET enlève \"Plot:\"\ndf['plot_text'] = s.str.replace(r'(?is)^.*?plot\\s*:\\s*', '', regex=True).str.strip()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.164089Z",
     "iopub.status.idle": "2025-11-06T13:11:18.164427Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.164238Z",
     "shell.execute_reply": "2025-11-06T13:11:18.164252Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "55d6dff2a0d53cba",
   "cell_type": "code",
   "source": "print(df['plot_text'][0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.164880Z",
     "iopub.status.idle": "2025-11-06T13:11:18.165155Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.165008Z",
     "shell.execute_reply": "2025-11-06T13:11:18.165020Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "b41ce462d1600302",
   "cell_type": "code",
   "source": "# supprime \"Note...\" et tout ce qui suit\ndf['plot_only'] = (df['plot_text']\n                   .str.replace(r'(?is)\\bnote\\b.*$', '', regex=True)\n                   .str.strip(\" \\n\\r\\t:.\"))  # nettoie la fin",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.167352Z",
     "iopub.status.idle": "2025-11-06T13:11:18.167820Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.167619Z",
     "shell.execute_reply": "2025-11-06T13:11:18.167636Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "81b5897f57413628",
   "cell_type": "code",
   "source": "\nprint(df['plot_only'][0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.168870Z",
     "iopub.status.idle": "2025-11-06T13:11:18.169213Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.169037Z",
     "shell.execute_reply": "2025-11-06T13:11:18.169051Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "6c5a94736c32bbc0",
   "cell_type": "code",
   "source": "df = df.drop('plot_text', axis=1)\nprint(df.iloc[0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.169993Z",
     "iopub.status.idle": "2025-11-06T13:11:18.170771Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.170599Z",
     "shell.execute_reply": "2025-11-06T13:11:18.170614Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "4fa80563b60b9e84",
   "cell_type": "markdown",
   "source": "etudie l'équilibre du dataset",
   "metadata": {}
  },
  {
   "id": "529f53a23556b7d4",
   "cell_type": "code",
   "source": "import re\nimport pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\n\n# ===== 1) Paramètres =====\nANSWER_COL = \"answer\"  # <-- adapte au nom réel de ta colonne\n\nALLOWED = [\n    \"drama\",\"comedy\",\"romance\",\"thriller\",\"crime\",\"action\",\"adventure\",\"horror\",\n    \"documentary\",\"mystery\",\"sci-fi\",\"fantasy\",\"family\",\"biography\",\"war\",\"history\",\n    \"music\",\"animation\",\"musical\",\"western\",\"sport\",\"short\",\"film-noir\"\n]\n\n# ===== 2) Normalisation / parsing =====\ndef normalize_label(lbl: str) -> str:\n    x = lbl.strip().lower()\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = x.replace(\"documentry\",\"documentary\")\n    x = x.replace(\"scifi\",\"sci-fi\").replace(\"sci fi\",\"sci-fi\")\n    x = x.replace(\"film noir\",\"film-noir\")\n    return x\n\ndef parse_answer_to_labels(s: str):\n    if pd.isna(s):\n        return []\n    s = re.sub(r\"(?i)^answer\\s*:\\s*\", \"\", str(s).strip())\n    parts = [normalize_label(p) for p in s.split(\",\")]\n    # supprimer vides et doublons en gardant l'ordre\n    seen, out = set(), []\n    for p in parts:\n        if p and p not in seen:\n            seen.add(p); out.append(p)\n    return out\n\n# Liste de labels par ligne\ndf[\"labels_list\"] = df[ANSWER_COL].apply(parse_answer_to_labels)\n\n# ===== 3) Fréquences globales =====\n# Filtrer sur les labels autorisés (et compter les inconnus à part)\nall_labels = [g for row in df[\"labels_list\"] for g in row]\nunknown = sorted({g for g in all_labels if g not in ALLOWED})\nprint(\"Labels inconnus détectés :\", unknown)\n\nlabels_allowed = [g for g in all_labels if g in ALLOWED]\nfreq = pd.Series(labels_allowed).value_counts().reindex(ALLOWED, fill_value=0)\n\nprint(\"\\nTop fréquences (globales) :\")\nprint(freq.sort_values(ascending=False).head(10))\n\n# Export CSV\nfreq.to_frame(\"count\").to_csv(\"../dataset/genre_frequencies.csv\")\n\n# ===== 4) Répartition du nombre de genres par échantillon =====\ndf[\"n_labels\"] = df[\"labels_list\"].apply(lambda x: sum(1 for g in x if g in ALLOWED))\ndist = df[\"n_labels\"].value_counts().sort_index()\nprint(\"\\nRépartition du nombre de genres par échantillon :\")\nprint(dist)\n\n# Export\ndist.to_frame(\"num_samples\").to_csv(\"../dataset/labels_per_sample_distribution.csv\")\n\n# ===== 5) (Optionnel) Matrice de co-occurrence =====\n# Compte des paires (g_i, g_j) apparaissant ensemble\npair_counter = Counter()\nfor row in df[\"labels_list\"]:\n    row_clean = [g for g in row if g in ALLOWED]\n    for a, b in combinations(sorted(set(row_clean)), 2):\n        pair_counter[(a, b)] += 1\n\n# Construire la matrice carrée |ALLOWED| x |ALLOWED|\ncooc = pd.DataFrame(0, index=ALLOWED, columns=ALLOWED, dtype=int)\nfor (a, b), c in pair_counter.items():\n    cooc.at[a, b] = c\n    cooc.at[b, a] = c  # symétrique\n# diagonale = fréquence du label (optionnel)\nfor g in ALLOWED:\n    cooc.at[g, g] = freq.loc[g]\n\nimport numpy as np\n\nprint(\"\\nPaires les plus fréquentes :\")\n# masque du triangle supérieur (sans diagonale)\nmask = np.triu(np.ones(cooc.shape, dtype=bool), k=1)\n\npairs_df = (\n    cooc.where(mask)        # garde uniquement au-dessus de la diagonale\n        .stack()            # enlève les NaN et empile (g1,g2,count)\n        .reset_index()\n        .rename(columns={\"level_0\":\"g1\", \"level_1\":\"g2\", 0:\"count\"})\n        .sort_values(\"count\", ascending=False)\n)\n\nprint(pairs_df.head(10))\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.172452Z",
     "iopub.status.idle": "2025-11-06T13:11:18.172788Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.172614Z",
     "shell.execute_reply": "2025-11-06T13:11:18.172627Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "3746f7ed5cb5a4ac",
   "cell_type": "markdown",
   "source": "Etude pour possiblement supprimé les réppoinses avec trop de labels",
   "metadata": {}
  },
  {
   "id": "d75f982ddd27be26",
   "cell_type": "code",
   "source": "# Filtrer les échantillons avec au moins 8 genres (n >= 8)\nmask = df['n_labels'].ge(8)\nsubset = df.loc[mask, ['n_labels', 'labels_list']].copy()\n\n# (optionnel) si tu as une colonne id, ajoute-la à l'affichage/export :\n# subset = df.loc[mask, ['id', 'n_labels', 'labels_list']].copy()\n\n# Aperçu\nprint(\"Nombre d'échantillons avec n_labels >= 8 :\", subset.shape[0])\nprint(subset.sort_values('n_labels', ascending=False).head(10))\n\n# Répartition (pour n >= 8)\nprint(\"\\nRépartition pour n >= 8 :\")\nprint(subset['n_labels'].value_counts().sort_index())\n\n# Export CSV (adapte le chemin si besoin)\nsubset.to_csv(\"../dataset/samples_with_nlabels_ge_8.csv\", index=False)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.173349Z",
     "iopub.status.idle": "2025-11-06T13:11:18.173680Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.173502Z",
     "shell.execute_reply": "2025-11-06T13:11:18.173514Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "f4761e01b85a0bdc",
   "cell_type": "markdown",
   "source": "Dataset unbalanced voir si l'entrainement est compliqué possiblement possible de supprimé les lignes avec trop de labels",
   "metadata": {}
  },
  {
   "id": "38feafe5f8730755",
   "cell_type": "markdown",
   "source": "préparation des onehots et études des tokens.",
   "metadata": {}
  },
  {
   "id": "3a5841b324a9328e",
   "cell_type": "code",
   "source": "import re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# --- classes cibles (ordre figé) ---\nALLOWED = [\n    \"drama\",\"comedy\",\"romance\",\"thriller\",\"crime\",\"action\",\"adventure\",\"horror\",\n    \"documentary\",\"mystery\",\"sci-fi\",\"fantasy\",\"family\",\"biography\",\"war\",\"history\",\n    \"music\",\"animation\",\"musical\",\"western\",\"sport\",\"short\",\"film-noir\"\n]\nANSWER_COL = \"answer\"  # adapte si besoin\n\n# --- parsing & normalisation ---\ndef _norm(lbl: str) -> str:\n    x = str(lbl).strip().lower()\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = x.replace(\"documentry\",\"documentary\")\n    x = x.replace(\"scifi\",\"sci-fi\").replace(\"sci fi\",\"sci-fi\")\n    x = x.replace(\"film noir\",\"film-noir\")\n    return x\n\ndef parse_labels(s):\n    if pd.isna(s): return []\n    parts = [_norm(p) for p in str(s).split(\",\")]\n    # uniques, non vides\n    seen, out = set(), []\n    for p in parts:\n        if p and p not in seen:\n            seen.add(p); out.append(p)\n    # on garde seulement les labels autorisés\n    out = [p for p in out if p in ALLOWED]\n    return out\n\n# 1) liste de labels par ligne\ndf[\"labels_list\"] = df[ANSWER_COL].apply(parse_labels)\n\n# 2) one-hot -> une SEULE colonne 'labels' (liste float32)\nmlb = MultiLabelBinarizer(classes=ALLOWED)\nmlb.fit([ALLOWED])  # fige l'ordre\nY = mlb.transform(df[\"labels_list\"]).astype(np.float32)  # (N, num_labels)\n\ndf[\"labels\"] = [row.tolist() for row in Y]  # chaque ligne: [0.,1.,0.,...,0.]\n\n# (optionnel) mapping utile pour l'entraînement/inférence\nid2label = {i: lab for i, lab in enumerate(ALLOWED)}\nlabel2id = {lab: i for i, lab in enumerate(ALLOWED)}\n\n# (optionnel) nettoyer d'éventuelles colonnes one-hot déjà créées\n# df.drop(columns=ALLOWED, errors='ignore', inplace=True)\n\n# (optionnel) aperçu\nprint(df[[\"labels_list\",\"labels\"]].head(3).to_string(index=False))\nprint(\"num_labels =\", len(ALLOWED))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.175513Z",
     "iopub.status.idle": "2025-11-06T13:11:18.175832Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.175665Z",
     "shell.execute_reply": "2025-11-06T13:11:18.175679Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "d18770cc25c3927c",
   "cell_type": "code",
   "source": "print(df.head())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.177221Z",
     "iopub.status.idle": "2025-11-06T13:11:18.177721Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.177411Z",
     "shell.execute_reply": "2025-11-06T13:11:18.177425Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "155ddaa73da0bdab",
   "cell_type": "code",
   "source": "# --- Sauvegarder les mappings id2label / label2id en .txt ---\n\n# Dossier/chemins de sortie (modifie si besoin)\nid2label_path = \"../dataset/id2label.txt\"\nlabel2id_path = \"../dataset/label2id.txt\"\n\n# 1) id2label : une ligne \"id<TAB>label\"\nwith open(id2label_path, \"w\", encoding=\"utf-8\") as f:\n    for i, lab in enumerate(ALLOWED):\n        f.write(f\"{i}\\t{lab}\\n\")\n\n# 2) label2id : une ligne \"label<TAB>id\"\nwith open(label2id_path, \"w\", encoding=\"utf-8\") as f:\n    for i, lab in enumerate(ALLOWED):\n        f.write(f\"{lab}\\t{i}\\n\")\n\nprint(\"Fichiers créés :\")\nprint(\" -\", id2label_path)\nprint(\" -\", label2id_path)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.178635Z",
     "iopub.status.idle": "2025-11-06T13:11:18.178940Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.178789Z",
     "shell.execute_reply": "2025-11-06T13:11:18.178802Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "cc626a9c1968ddc3",
   "cell_type": "code",
   "source": "# --- Histogramme du nombre de tokens par phrase (BERT) ---\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer\nimport matplotlib.pyplot as plt\n\n# Paramètres\nTEXT_COL = \"plot_only\"            # <- remplace si nécessaire\nMODEL_NAME = \"bert-base-uncased\"  # <- remplace par le tokenizer de ton modèle\nMAX_SAMPLES = None                # ex: 20000 pour échantillonner, None = tout\n\n# 1) Récupérer les textes\ntexts = df[TEXT_COL].fillna(\"\").astype(str)\nif MAX_SAMPLES is not None and len(texts) > MAX_SAMPLES:\n    texts = texts.sample(MAX_SAMPLES, random_state=42)\n\n# 2) Tokenizer (BERT)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Encodage batched + longueur\nenc = tokenizer(\n    texts.tolist(),\n    add_special_tokens=True,\n    truncation=False,        # on veut la vraie longueur sans couper\n    padding=False,\n    return_length=True\n)\nlengths = np.array(enc[\"length\"])\n\n# 3) Statistiques utiles\ndef p(q): return np.percentile(lengths, q)\nprint(\"Nombre d'exemples :\", len(lengths))\nprint(f\"min={lengths.min()}  max={lengths.max()}  mean={lengths.mean():.1f}  median={np.median(lengths):.1f}\")\nprint(f\"p75={p(75):.0f}  p90={p(90):.0f}  p95={p(95):.0f}  p99={p(99):.0f}\")\n\n# 4) Histogramme\nplt.figure(figsize=(8,4.5))\nplt.hist(lengths, bins=50)\nplt.title(\"Distribution du nombre de tokens (BERT)\")\nplt.xlabel(\"Tokens par phrase\")\nplt.ylabel(\"Fréquence\")\nplt.tight_layout()\nplt.show()\n\n# 5) (optionnel) Export csv des longueurs\n# pd.Series(lengths, name=\"num_tokens\").to_csv(\"../dataset/token_lengths.csv\", index=False)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.179413Z",
     "iopub.status.idle": "2025-11-06T13:11:18.180092Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.179540Z",
     "shell.execute_reply": "2025-11-06T13:11:18.179552Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "945cb77ebd79b030",
   "cell_type": "code",
   "source": "\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\n\nTEXT_COL = \"plot_only\"\n# df[\"labels\"] = liste de 0/1 (float) de longueur = num_labels\n\n# 1) Pandas -> HF Dataset\nds = Dataset.from_pandas(df[[TEXT_COL, \"labels\"]].rename(columns={TEXT_COL:\"text\"}), preserve_index=False)\n\n# 2) Tokenizer\ntok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ndef tok_fn(batch):\n    return tok(batch[\"text\"], truncation=True, max_length=256)\n\nds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\nds_tok = ds_tok.with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.181799Z",
     "iopub.status.idle": "2025-11-06T13:11:18.182182Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.181988Z",
     "shell.execute_reply": "2025-11-06T13:11:18.182004Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "a571a40fb6ff8ab5",
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom math import ceil\nfrom collections import Counter\n\n# ---- paramètres ----\nLABELS_COL = \"labels\"   # chaque ligne: liste/array de 0/1 de taille L\nTEST_RATIO = 0.20\nMIN_PER_LABEL_TEST = 5\nRANDOM_STATE = 42\n\nrng = np.random.default_rng(RANDOM_STATE)\n\n# ---- préparer Y (N, L) ----\nY = np.vstack(df[LABELS_COL].apply(lambda x: np.asarray(x, dtype=int)).to_numpy())  # (N,L)\nN, L = Y.shape\ntest_target_size = ceil(TEST_RATIO * N)\n\n# ---- totaux & exigences par label ----\ntotals = Y.sum(axis=0)                       # (L,)\nrequired = np.minimum(totals, MIN_PER_LABEL_TEST).astype(int)\n\n# faisabilité grossière : si les labels ne co-apparaissent pas, il faut au moins sum(required) échantillons\nmin_possible = int(required.sum() > 0)  # borne triviale; on fera un check pratique ci-dessous\n\n# ---- sélection gourmande pour couvrir les exigences ----\nall_idx = np.arange(N)\nremaining = set(all_idx.tolist())\ntest_idx = []\n\nreq_left = required.copy()  # combien il manque encore par label\n\n# boucle: ajouter l'échantillon qui couvre le + de besoins restants\nwhile (req_left > 0).any() and len(test_idx) < test_target_size and remaining:\n    # score = nb de labels déficitaires couverts par chaque sample\n    deficit = (req_left > 0).astype(int)              # (L,)\n    # évaluer un batch de candidats pour éviter O(N^2)\n    cand_list = list(remaining)\n    Yc = Y[cand_list]                                  # (#cand, L)\n    scores = (Yc * deficit).sum(axis=1)               # (#cand,)\n    if scores.max() == 0:\n        break  # plus aucun sample ne couvre des déficits restants\n    # choisir le meilleur (si ex-aequo -> aléatoire stable)\n    best_idxs = np.where(scores == scores.max())[0]\n    pick_local = rng.choice(best_idxs)\n    pick = cand_list[pick_local]\n\n    # ajouter au test\n    test_idx.append(pick)\n    remaining.remove(pick)\n    # mettre à jour req_left\n    req_left -= Y[pick]\n    req_left = np.maximum(req_left, 0)\n\n# Si on n'a pas réussi à satisfaire toutes les exigences, prévenir proprement\nif (req_left > 0).any():\n    # labels impossibles à satisfaire davantage (typiquement parce qu'on a atteint test_target_size\n    # ou aucune combinaison ne couvre les déficits restants)\n    unmet = np.where(req_left > 0)[0].tolist()\n    # Conseil: augmenter TEST_RATIO ou réduire MIN_PER_LABEL_TEST\n    print(f\"⚠️ Impossible d'atteindre MIN_PER_LABEL_TEST={MIN_PER_LABEL_TEST} pour tous les labels \"\n          f\"(test déjà {len(test_idx)}/{test_target_size}). \"\n          f\"Labels avec déficit restant: {unmet}\")\n\n# ---- compléter le test jusqu'à la taille visée, de façon (grossièrement) stratifiée ----\n# On vise une proportion ~ TEST_RATIO par label\ndesired_test_counts = np.floor(totals * TEST_RATIO).astype(int)\ndesired_test_counts = np.maximum(desired_test_counts, required)  # au moins 'required'\ncurrent_counts = Y[test_idx].sum(axis=0) if test_idx else np.zeros(L, dtype=int)\n\n# on remplit tant qu'il reste de la place\ncand_list = list(remaining)\nrng.shuffle(cand_list)\n\ndef gain(i):\n    # combien cet échantillon rapproche des objectifs (labels en déficit)\n    add = np.maximum(0, desired_test_counts - current_counts)\n    return (Y[i] * (add > 0)).sum()\n\nfor i in cand_list:\n    if len(test_idx) >= test_target_size:\n        break\n    # priorité à ceux qui comblent des déficits\n    if gain(i) > 0:\n        test_idx.append(i)\n        current_counts += Y[i]\n# si toujours pas assez, compléter aléatoirement\nif len(test_idx) < test_target_size:\n    fill = [i for i in remaining if i not in set(test_idx)]\n    rng.shuffle(fill)\n    need = test_target_size - len(test_idx)\n    test_idx.extend(fill[:need])\n\ntest_idx = np.array(test_idx, dtype=int)\ntrain_mask = np.ones(N, dtype=bool)\ntrain_mask[test_idx] = False\ntrain_idx = np.where(train_mask)[0]\n\n# ---- construire les splits ----\ntrain_df = df.iloc[train_idx].reset_index(drop=True)\ntest_df  = df.iloc[test_idx].reset_index(drop=True)\n\nprint(f\"Split: train={len(train_df)}  test={len(test_df)} (visé ~{test_target_size})\")\n\n# ---- vérifications ----\ntest_counts = np.vstack(test_df[LABELS_COL].apply(lambda x: np.asarray(x, dtype=int))).sum(axis=0)\nmin_test_per_label = test_counts.min() if L else 0\nsummary = pd.DataFrame({\n    \"total\": totals,\n    \"required_min\": required,\n    \"in_test\": test_counts\n})\nprint(summary)\n\n# (optionnel) sauver\n# train_df.to_parquet(\"../dataset/train.parquet\", compression=\"snappy\")\n# test_df.to_parquet(\"../dataset/test.parquet\", compression=\"snappy\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:11:18.182827Z",
     "iopub.status.idle": "2025-11-06T13:11:18.183158Z",
     "shell.execute_reply.started": "2025-11-06T13:11:18.182972Z",
     "shell.execute_reply": "2025-11-06T13:11:18.182987Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "419023f22e80964a",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
