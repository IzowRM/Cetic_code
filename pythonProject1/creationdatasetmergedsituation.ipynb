{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 13522720,
     "sourceType": "datasetVersion",
     "datasetId": 8586397
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "print('Chargement du ds')\n",
    "ds = load_dataset(\"parquet\", data_files=\"data/mmimdb_merged.parquet\", split=\"train[:100%]\")\n",
    "print(ds[0])"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Aplanissement du ds ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def resize(img):\n    max_size = (224, 224)  # taille max (width, height)\n    img.thumbnail(max_size, Image.Resampling.LANCZOS)\n    return img",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import re\ndef extract_plot(text: str) -> str:\n    PATTERN = re.compile(r'(?is)\\bplot\\s*:\\s*(.*?)(?=\\bnote\\s+that\\b|answer\\s*:|$)')\n    if text is None:\n        return None\n    m = PATTERN.search(text)\n    if not m:\n        return \"\"  # ou text.strip() si tu préfères conserver l'original quand Pas de \"Plot:\"\n    plot = m.group(1).strip()\n    # petites normalisations\n    plot = re.sub(r'\\s+\\n', '\\n', plot)\n    plot = re.sub(r'\\s{2,}', ' ', plot)\n    return plot\n    ",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Creation du one-hot \n",
    "def data_loader(ds_flat):\n",
    "    LABELS = [\n",
    "        \"drama\", \"comedy\", \"romance\", \"thriller\", \"crime\", \"action\", \"adventure\", \"horror\",\n",
    "        \"documentary\", \"mystery\", \"sci-fi\", \"fantasy\", \"family\", \"biography\", \"war\", \"history\",\n",
    "        \"music\", \"animation\", \"musical\", \"western\", \"sport\", \"short\", \"film-noir\"\n",
    "    ]\n",
    "    label2id = {lab: i for i, lab in enumerate(LABELS)}\n",
    "\n",
    "    ALIASES = {\n",
    "        \"documentry\": \"documentary\",\n",
    "        \"science fiction\": \"sci-fi\",\n",
    "        \"sci fi\": \"sci-fi\",\n",
    "        \"film noir\": \"film-noir\",\n",
    "        \"westerns\": \"western\",\n",
    "        \"sports\": \"sport\",\n",
    "    }\n",
    "\n",
    "    def norm_token(t: str) -> str:\n",
    "        t = t.lower()\n",
    "        t = re.sub(r\"[\\s\\-]+\", \" \", t).strip()\n",
    "        t = ALIASES.get(t, t)\n",
    "        t = t.replace(\"sci fi\", \"sci-fi\")\n",
    "        return t\n",
    "\n",
    "    def parse_answer(ans: str):\n",
    "        if ans is None:\n",
    "            return set()\n",
    "        toks = re.split(r\"[;,]\", ans)\n",
    "        toks = [norm_token(x) for x in toks if x.strip()]\n",
    "        return set(t for t in toks if t in label2id)\n",
    "\n",
    "    def to_multi_hot_float(labels_set):\n",
    "        vec = [0.0] * len(LABELS)\n",
    "        for lab in labels_set:\n",
    "            vec[label2id[lab]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def mapper(batch):\n",
    "        answers = batch[\"answer\"]\n",
    "        return {\"labels\": [to_multi_hot_float(parse_answer(a)) for a in answers]}\n",
    "\n",
    "    new_features = ds_flat.features.copy()\n",
    "    new_features[\"labels\"] = Sequence(feature=Value(\"float32\"), length=len(LABELS))\n",
    "\n",
    "    ds_flat = ds_flat.map(mapper, batched=True, features=new_features)\n",
    "    print(type(ds_flat[0][\"labels\"][0]), ds_flat.features[\"labels\"])\n",
    "\n",
    "    return ds_flat\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import re\n\nLABELS = [\n    \"drama\", \"comedy\", \"romance\", \"thriller\", \"crime\", \"action\", \"adventure\", \"horror\",\n    \"documentary\", \"mystery\", \"sci-fi\", \"fantasy\", \"family\", \"biography\", \"war\", \"history\",\n    \"music\", \"animation\", \"musical\", \"western\", \"sport\", \"short\", \"film-noir\"\n]\nlabel2id = {lab: i for i, lab in enumerate(LABELS)}\n\nALIASES = {\n    \"documentry\": \"documentary\",\n    \"science fiction\": \"sci-fi\",\n    \"sci fi\": \"sci-fi\",\n    \"film noir\": \"film-noir\",\n    \"westerns\": \"western\",\n    \"sports\": \"sport\",\n}\n\ndef norm_token(t: str) -> str:\n    t = t.lower()\n    t = re.sub(r\"[\\s\\-]+\", \" \", t).strip()\n    t = ALIASES.get(t, t)\n    t = t.replace(\"sci fi\", \"sci-fi\")\n    return t\n\ndef parse_answer(ans: str):\n    if ans is None:\n        return set()\n    toks = re.split(r\"[;,]\", ans)\n    toks = [norm_token(x) for x in toks if x.strip()]\n    return set(t for t in toks if t in label2id)\n\ndef to_multi_hot_float(labels_set):\n    vec = [0.0] * len(LABELS)\n    for lab in labels_set:\n        vec[label2id[lab]] = 1.0\n    return vec\n\ndef data_loader(answer: str):\n    labels_set = parse_answer(answer)\n    return to_multi_hot_float(labels_set)\n",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "def general_dataset_transform(example):\n",
    "    import json\n",
    "    Image.MAX_IMAGE_PIXELS = 200_000_000\n",
    "    msgs = example.get(\"messages\", []) or []\n",
    "    user_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"]\n",
    "    assistant_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"]\n",
    "    \n",
    "    image = resize(example.get(\"images\")[0])\n",
    "    image = pad_to_224_width(image)\n",
    "    prompt = \"\\n\\n\".join([u for u in user_msgs if isinstance(u, str)]) if user_msgs else \"\"\n",
    "    plot = extract_plot(prompt)\n",
    "    answer = assistant_msgs[-1] if assistant_msgs else \"\"\n",
    "    \n",
    "    labels_vec = data_loader(answer)  \n",
    "\n",
    "    return {\n",
    "        \"plot\": plot,\n",
    "        \"answer\": answer,\n",
    "        \"image\": image,\n",
    "        \"labels\": labels_vec, \n",
    "    }\n",
    "    \n",
    "ds_flat = ds.map(general_dataset_transform, remove_columns=ds.column_names)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"plot\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True)\n",
    "\n",
    "print('step1')\n",
    "Dataset_tokenized_images = ds_flat.map(tokenize_function, batched=True)\n",
    "Dataset_tokenized_images = Dataset_tokenized_images.map(pad_to_224_width)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(ds_flat[0][\"answer\"])\n",
    "print(ds_flat[0][\"labels\"])\n",
    "print(len(ds_flat[0][\"labels\"]))\n",
    "print(ds_flat[0][\"image\"])\n",
    "print(ds_flat[0][\"plot\"])"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "Dataset_tokenized_images.save_to_disk(\"data/Dataset_tokenized_images\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
